{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oppg_a.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Magnusrm/TDAT3025-Machine-Learning/blob/master/Task2.2/oppg_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lSeE1JXOSA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efe579ab-ad52-4314-de70-734ab8b225d4"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "char_encodings = [\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0],  # ' '\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0],  # 'h'\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0],  # 'e'\n",
        "    [0, 0, 0, 1, 0, 0, 0, 0],  # 'l'\n",
        "    [0, 0, 0, 0, 1, 0, 0, 0],  # 'o'\n",
        "    [0, 0, 0, 0, 0, 1, 0, 0],  # 'w'\n",
        "    [0, 0, 0, 0, 0, 0, 1, 0],  # 'r'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1],  # 'd'\n",
        "]\n",
        "encoding_size = np.shape(char_encodings)[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
        "char_to_index = dict((char, i) for i, char in enumerate(index_to_char))\n",
        "\n",
        "x_train = [[[char_encodings[0], char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4],\n",
        "             char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7],\n",
        "             char_encodings[0], char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4],\n",
        "             char_encodings[0], char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7]]]]  # ' hello world hello world'\n",
        "y_train = [[[char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0],\n",
        "             char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0],\n",
        "             char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0],\n",
        "             char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]]]]  # 'hello world hello world '\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(128, input_shape=(None, encoding_size), return_sequences=True))\n",
        "model.add(tf.keras.layers.Dense(encoding_size, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.05)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=optimizer)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, data):\n",
        "    if epoch % 10 == 9:\n",
        "        print(\"epoch\", epoch)\n",
        "        print(\"loss\", data['loss'])\n",
        "\n",
        "        # Generate text from the initial text ' h'\n",
        "        text = ' h'\n",
        "        for i in range(50):\n",
        "            x = np.zeros((1, i + 2, encoding_size))\n",
        "            for t, char in enumerate(text):\n",
        "                x[0, t, char_to_index[char]] = 1\n",
        "            y = model.predict(x)[0][-1]\n",
        "            text += index_to_char[y.argmax()]\n",
        "        print(text)\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=500, verbose=False, callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 9\n",
            "loss 2.4682812690734863\n",
            " hellwwwwow  ddw  dd   do   dd   dd   do   dd   do  \n",
            "epoch 19\n",
            "loss 1.1387118101119995\n",
            " hello werld hlld hld hlld hlld hlld hlld hlld hlld \n",
            "epoch 29\n",
            "loss 2.483595371246338\n",
            " hello wo                                           \n",
            "epoch 39\n",
            "loss 0.10713532567024231\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 49\n",
            "loss 0.013945757411420345\n",
            " hello world hello world world world world world wor\n",
            "epoch 59\n",
            "loss 0.005174545571208\n",
            " hello world hello world world world world world wor\n",
            "epoch 69\n",
            "loss 0.002456281566992402\n",
            " hello world hello world world world world world wor\n",
            "epoch 79\n",
            "loss 0.00129110855050385\n",
            " hello world hello world world world world world wor\n",
            "epoch 89\n",
            "loss 0.0007047879043966532\n",
            " hello world hello world world world world world wor\n",
            "epoch 99\n",
            "loss 0.00038682649028487504\n",
            " hello world hello world world world world world wor\n",
            "epoch 109\n",
            "loss 0.00021607649978250265\n",
            " hello world hello world world world world world wor\n",
            "epoch 119\n",
            "loss 0.00012248381972312927\n",
            " hello world hello world world world world world wor\n",
            "epoch 129\n",
            "loss 6.79643708281219e-05\n",
            " hello world hello world world world world world wor\n",
            "epoch 139\n",
            "loss 3.85824496333953e-05\n",
            " hello world hello world world world world world wor\n",
            "epoch 149\n",
            "loss 2.216824759670999e-05\n",
            " hello world hello world world world world world wor\n",
            "epoch 159\n",
            "loss 1.284738118556561e-05\n",
            " hello world hello world world world world world wor\n",
            "epoch 169\n",
            "loss 7.438195552822435e-06\n",
            " hello world hello world world world world world wor\n",
            "epoch 179\n",
            "loss 4.333765446062898e-06\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 189\n",
            "loss 2.5605202154110884e-06\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 199\n",
            "loss 1.5298541029551416e-06\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 209\n",
            "loss 9.114548902289243e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 219\n",
            "loss 5.488596457325912e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 229\n",
            "loss 3.427267927236244e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 239\n",
            "loss 2.2600097793201712e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 249\n",
            "loss 1.6887985054836463e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 259\n",
            "loss 1.316269475637455e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 269\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 279\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 289\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 299\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 309\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 319\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 329\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 339\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 349\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 359\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 369\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 379\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 389\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 399\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 409\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 419\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 429\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 439\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 449\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 459\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 469\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 479\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 489\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n",
            "epoch 499\n",
            "loss 1.1920930376163597e-07\n",
            " hello world hello world hello world hello world hel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f41e11c34a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}