{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oppg_c.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Magnusrm/TDAT3025-Machine-Learning/blob/master/Task2.2/oppg_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YchdblqoWD6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e34ea6f-1591-418c-f725-7168d9a86715"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "char_encodings = [\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #0 ' '\n",
        "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #1 'r'\n",
        "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #2 'a'\n",
        "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #3 't'\n",
        "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #4 'c'\n",
        "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #5 'h'\n",
        "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #6 'f'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #7 'l'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #8 'm'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #9 'p'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  #10 's'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  #11 'o'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  #12 'n'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  #13 'ğŸ€'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  #14 'ğŸ©'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],  #15 'ğŸˆ'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  #16 'ğŸ¢'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  #17 'ğŸ‘¨'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],  #18 'ğŸ§¢'\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   #19 'ğŸ‘¦'\n",
        "]\n",
        "\n",
        "encoding_size = np.shape(char_encodings)[1]\n",
        "\n",
        "index_to_char = [' ', 'r', 'a', 't', 'c', 'h', 'f', 'l', 'm', 'p', 's', 'o', 'n', 'ğŸ€', 'ğŸ©', 'ğŸˆ', 'ğŸ¢', 'ğŸ‘¨', 'ğŸ§¢', 'ğŸ‘¦']\n",
        "\n",
        "char_to_index = dict((char, i) for i, char in enumerate(index_to_char))\n",
        "\n",
        "\n",
        "x_train = [[[char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[0]],     # 'rat '\n",
        "           [char_encodings[5], char_encodings[2], char_encodings[3], char_encodings[0]],      # 'hat '\n",
        "           [char_encodings[4], char_encodings[2], char_encodings[3], char_encodings[0]],      # 'cat '\n",
        "           [char_encodings[6], char_encodings[7], char_encodings[2], char_encodings[3]],      # 'flat'\n",
        "           [char_encodings[8], char_encodings[2], char_encodings[3], char_encodings[3]],      # 'matt'\n",
        "           [char_encodings[4], char_encodings[2], char_encodings[9], char_encodings[0]],      # 'cap '\n",
        "           [char_encodings[10], char_encodings[11], char_encodings[12], char_encodings[0]]]]  # 'son '\n",
        "\n",
        "y_train = [[[char_encodings[13]],  # 'ğŸ€'\n",
        "            [char_encodings[14]],  # 'ğŸ©'\n",
        "            [char_encodings[15]],  # 'ğŸˆ'\n",
        "            [char_encodings[16]],  # 'ğŸ¢'\n",
        "            [char_encodings[17]],  # 'ğŸ‘¨'\n",
        "            [char_encodings[18]],  # 'ğŸ§¢'\n",
        "            [char_encodings[19]]]] # 'ğŸ‘¦'\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(128, input_shape=(None, encoding_size), return_sequences=True))\n",
        "model.add(tf.keras.layers.Dense(encoding_size, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.05)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=optimizer)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, data):\n",
        "    if epoch % 100 == 99:\n",
        "        print(\"epoch\", epoch)\n",
        "        print(\"loss\", data['loss'])\n",
        "\n",
        "        # Generate emoji from chosen text\n",
        "        words = ['rat ', 'rats', 'rt  ', 'hat ', 'cat ', 'flat', 'matt', 'cap ', 'son ']\n",
        "        \n",
        "        for i, text in enumerate(words):\n",
        "            x = np.zeros((1, 4, encoding_size))\n",
        "            for t, char in enumerate(words[i]):\n",
        "                x[0, t, char_to_index[char]] = 1\n",
        "            y = model.predict(x)[0][-1]\n",
        "            text += index_to_char[y.argmax()]\n",
        "            print(text)\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=500, verbose=False, callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 99\n",
            "loss 0.1056723764964614\n",
            "rat ğŸ€\n",
            "ratsğŸ€\n",
            "rt  ğŸ€\n",
            "hat ğŸ©\n",
            "cat ğŸˆ\n",
            "flatğŸ¢\n",
            "mattğŸ‘¨\n",
            "cap ğŸ§¢\n",
            "son ğŸ‘¦\n",
            "epoch 199\n",
            "loss 0.10534063407354008\n",
            "rat ğŸ€\n",
            "ratsğŸ€\n",
            "rt  ğŸ€\n",
            "hat ğŸ©\n",
            "cat ğŸˆ\n",
            "flatğŸ¢\n",
            "mattğŸ‘¨\n",
            "cap ğŸ§¢\n",
            "son ğŸ‘¦\n",
            "epoch 299\n",
            "loss 0.10538812620300218\n",
            "rat ğŸ€\n",
            "ratsğŸ€\n",
            "rt  ğŸ€\n",
            "hat ğŸ©\n",
            "cat ğŸˆ\n",
            "flatğŸ¢\n",
            "mattğŸ‘¨\n",
            "cap ğŸ§¢\n",
            "son ğŸ‘¦\n",
            "epoch 399\n",
            "loss 0.10495736343521044\n",
            "rat ğŸ€\n",
            "ratsğŸ€\n",
            "rt  ğŸ€\n",
            "hat ğŸ©\n",
            "cat ğŸˆ\n",
            "flatğŸ¢\n",
            "mattğŸ‘¨\n",
            "cap ğŸ§¢\n",
            "son ğŸ‘¦\n",
            "epoch 499\n",
            "loss 0.10515048674175732\n",
            "rat ğŸ€\n",
            "ratsğŸ€\n",
            "rt  ğŸ€\n",
            "hat ğŸ©\n",
            "cat ğŸˆ\n",
            "flatğŸ¢\n",
            "mattğŸ‘¨\n",
            "cap ğŸ§¢\n",
            "son ğŸ‘¦\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3da81ca90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}